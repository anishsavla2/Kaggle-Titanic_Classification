{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-04T19:12:54.690699Z","iopub.execute_input":"2023-10-04T19:12:54.691056Z","iopub.status.idle":"2023-10-04T19:12:55.063676Z","shell.execute_reply.started":"2023-10-04T19:12:54.691026Z","shell.execute_reply":"2023-10-04T19:12:55.062287Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2023-10-04T19:12:57.717022Z","iopub.execute_input":"2023-10-04T19:12:57.717831Z","iopub.status.idle":"2023-10-04T19:12:58.375154Z","shell.execute_reply.started":"2023-10-04T19:12:57.717797Z","shell.execute_reply":"2023-10-04T19:12:58.374043Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load the data\ntrain_data = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_data = pd.read_csv('/kaggle/input/titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-10-04T19:12:59.851249Z","iopub.execute_input":"2023-10-04T19:12:59.852364Z","iopub.status.idle":"2023-10-04T19:12:59.871622Z","shell.execute_reply.started":"2023-10-04T19:12:59.852320Z","shell.execute_reply":"2023-10-04T19:12:59.870593Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Combining both train and test data sets to perform feature engineering and then divide them later again.\n# This ensures consistency in the number of columns in both datasets after one-hot encoding and other operations.\ncombined_data = [train_data, test_data]","metadata":{"execution":{"iopub.status.busy":"2023-10-04T19:13:01.204757Z","iopub.execute_input":"2023-10-04T19:13:01.205113Z","iopub.status.idle":"2023-10-04T19:13:01.211100Z","shell.execute_reply.started":"2023-10-04T19:13:01.205086Z","shell.execute_reply":"2023-10-04T19:13:01.209888Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"### 1. Handling Missing Data:\n\n# Filling missing Age values with the median of the Age values.\nfor dataset in combined_data:\n    dataset['Age'].fillna(dataset['Age'].median(), inplace=True)\n\n# Filling missing Embarked values with the most frequent value.\nfor dataset in combined_data:\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace=True)\n\n# Filling missing Fare values with the median of the Fare values.\nfor dataset in combined_data:\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace=True)\n\n# Handling Cabin data: Create a new feature 'HasCabin' that shows if someone had a Cabin or not.\nfor dataset in combined_data:\n    dataset['HasCabin'] = dataset[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-04T19:13:03.022952Z","iopub.execute_input":"2023-10-04T19:13:03.023299Z","iopub.status.idle":"2023-10-04T19:13:03.037571Z","shell.execute_reply.started":"2023-10-04T19:13:03.023273Z","shell.execute_reply":"2023-10-04T19:13:03.036764Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\n### 2. Feature Transformation and Creation:\n\n# Extract titles from Name and create a new feature 'Title'.\nfor dataset in combined_data:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# Group all non-common titles into one single grouping \"Rare\".\nfor dataset in combined_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\n# Convert categorical features to numeric type.\nfor dataset in combined_data:\n    dataset['Sex'] = dataset['Sex'].map({'female': 1, 'male': 0}).astype(int)\n    dataset['Title'] = dataset['Title'].map({\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}).astype(int)\n    dataset['Embarked'] = dataset['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\n\n# It's a good idea to convert 'Fare' from float to int type.\nfor dataset in combined_data:\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\n# You can also create a new feature 'FamilySize' combining Parch and SibSp.\nfor dataset in combined_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\n# Create 'IsAlone' feature based on 'FamilySize'.\nfor dataset in combined_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1","metadata":{"execution":{"iopub.status.busy":"2023-10-04T19:13:04.944040Z","iopub.execute_input":"2023-10-04T19:13:04.944938Z","iopub.status.idle":"2023-10-04T19:13:04.973593Z","shell.execute_reply.started":"2023-10-04T19:13:04.944905Z","shell.execute_reply":"2023-10-04T19:13:04.971815Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"test_passenger_id = test_data[\"PassengerId\"].copy()  # Add this line before dropping columns\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp', 'Parch']\ntrain_data = train_data.drop(drop_elements, axis=1)\ntest_data = test_data.drop(drop_elements, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T19:13:07.871123Z","iopub.execute_input":"2023-10-04T19:13:07.871784Z","iopub.status.idle":"2023-10-04T19:13:07.882336Z","shell.execute_reply.started":"2023-10-04T19:13:07.871731Z","shell.execute_reply":"2023-10-04T19:13:07.881166Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(train_data.head())","metadata":{"execution":{"iopub.status.busy":"2023-10-04T19:13:53.512010Z","iopub.execute_input":"2023-10-04T19:13:53.512383Z","iopub.status.idle":"2023-10-04T19:13:53.525295Z","shell.execute_reply.started":"2023-10-04T19:13:53.512358Z","shell.execute_reply":"2023-10-04T19:13:53.524157Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"   Survived  Pclass  Sex   Age  Fare  Embarked  HasCabin  Title  FamilySize  \\\n0         0       3    0  22.0     7         0         0      1           2   \n1         1       1    1  38.0    71         1         1      3           2   \n2         1       3    1  26.0     7         0         0      2           1   \n3         1       1    1  35.0    53         0         1      3           2   \n4         0       3    0  35.0     8         0         0      1           1   \n\n   IsAlone  \n0        0  \n1        0  \n2        1  \n3        0  \n4        1  \n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.svm import SVC\n# Splitting the dataset into training and testing sets\nX_train = train_data.drop(\"Survived\", axis=1)\ny_train = train_data[\"Survived\"]\nX_test = test_data  # We don't have a y_test since that's what you'd predict for the Kaggle submission.\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Using SVM\nsvm = SVC(kernel='linear', C=1)\nsvm.fit(X_train, y_train)\ny_pred = svm.predict(X_val)\n\n# Check accuracy on validation set\nacc_svm = accuracy_score(y_val, y_pred)\nprint(f\"SVM Accuracy: {acc_svm}\")\n\n# For submission to Kaggle\nsubmission_predictions = svm.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T19:14:09.970264Z","iopub.execute_input":"2023-10-04T19:14:09.971178Z","iopub.status.idle":"2023-10-04T19:14:11.728692Z","shell.execute_reply.started":"2023-10-04T19:14:09.971148Z","shell.execute_reply":"2023-10-04T19:14:11.727547Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"SVM Accuracy: 0.7988826815642458\n","output_type":"stream"}]},{"cell_type":"code","source":"submission = pd.DataFrame({\n    \"PassengerId\": test_passenger_id,  # Use the stored passenger IDs here\n    \"Survived\": submission_predictions\n})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T19:14:14.394169Z","iopub.execute_input":"2023-10-04T19:14:14.395203Z","iopub.status.idle":"2023-10-04T19:14:14.402489Z","shell.execute_reply.started":"2023-10-04T19:14:14.395168Z","shell.execute_reply":"2023-10-04T19:14:14.401413Z"},"trusted":true},"execution_count":10,"outputs":[]}]}